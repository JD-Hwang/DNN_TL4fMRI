{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a96099",
   "metadata": {},
   "source": [
    "# HSC transfer learning code\n",
    "### This is transfer learning code based on pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e9795",
   "metadata": {},
   "source": [
    "## Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c934d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as pth\n",
    "import torch\n",
    "import gc\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from torchsummary import summary\n",
    "from datetime import datetime as dt\n",
    "from pytz import timezone\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.utils.validation import check_array, check_random_state, _deprecate_positional_args\n",
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "tz = timezone('Asia/Seoul')\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d488bd2",
   "metadata": {},
   "source": [
    "## Set parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106d8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    ### Setting gpu number to use for training\n",
    "    'gpu_num': 2,\n",
    "    \n",
    "    ### Target task to classify\n",
    "    'target_task': 4,\n",
    "    'task_list': ['WM', 'MOTOR', 'EMOTION', 'RELATIONAL', 'SOCIAL', 'LANGUAGE', 'GAMBLING'],\n",
    "    'classes_for_task' : [8,5,2,2,2,2,2],\n",
    "    'output_size' : 0, #Output size will be designated accordingly later\n",
    "    \n",
    "    ### Number of subjects to train\n",
    "    ### If you want to use total subject, just set 2000\n",
    "    'train_n_subject': 20,\n",
    "    'test_n_subject': 20,\n",
    "\n",
    "\n",
    "    ### Set max beta, beta learning rate, etc.\n",
    "    'hsc':{\n",
    "        'type':'layer',\n",
    "        'max_b': [1e-3, 1e-3],\n",
    "        'b_lr': [1e-4, 1e-4], #3*1e-5\n",
    "        'l2': 5*1e-4, \n",
    "        'l1': None,\n",
    "        'patience':1,\n",
    "    },\n",
    "    \n",
    "    ### Set hidden node combination\n",
    "    'comb':{\n",
    "        'comb_num': (5000,1000,),\n",
    "    },\n",
    "\n",
    "\n",
    "    ### Set activation function. Default to ReLU for all layers\n",
    "    'activation':'relu',\n",
    "    'activation2':'relu',\n",
    "\n",
    "    ### Set dropout ratio, batch size, Normalization method\n",
    "    'dropout_rate': 0.5,\n",
    "    'batch_size': 512,\n",
    "    'norm':{\n",
    "        'type':'batch',\n",
    "        'first': True,\n",
    "        'second':True,\n",
    "    },\n",
    "    \n",
    "    ### Set epochs to learn. This model has earlystopping method, so default setting to 2000.\n",
    "    'num_epoch': 5,\n",
    "    \n",
    "    ### Set learning rate, momentum\n",
    "    'learning_rate': 1e-3,\n",
    "    'momentum' : 0.9,\n",
    "\n",
    "    ### For cross-validation, set repeated group Kfold criteria.\n",
    "    'num_fold': 5, # The number of folds in one validation,\n",
    "    'num_repeat_of_fold': 10, # The number of trials of group Kfold\n",
    "\n",
    "    ### Set early stopping criteria\n",
    "    'es_patience': 200, # Model stops learning if there is no increasing in performance for this epochs on validation data\n",
    "    'lr_patience':30, # Model starts annealing if there is no increasing in performance for this epochs on validation data\n",
    "    'anneal_factor':0.3, # Annealing factor for ReduceLROnPlateau\n",
    "    \n",
    "    ### Set random seed for reproducibility\n",
    "    'random_state': 7777,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7df8707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Target task setting\n",
    "target_task_list = [config['task_list'][config['target_task']]] # 'WM', 'MOTOR', 'EMOTION', 'RELATIONAL', 'SOCIAL', 'LANGUAGE', 'GAMBLING'\n",
    "task_idx_dict = {task:i for i, task in enumerate(config['task_list'])}\n",
    "target_task_list = [task_idx_dict[target_task] for target_task in target_task_list]\n",
    "\n",
    "### Set output class size accordingly\n",
    "config['output_size'] = config['classes_for_task'][config['target_task']]    \n",
    "\n",
    "# Target hoyer sparsity list\n",
    "target_h_list = [\n",
    "\n",
    "#     [0.8, 0.3],\n",
    "    [0.8, 0.5],\n",
    "#     [0.8, 0.8],\n",
    "]\n",
    "\n",
    "# Set [1,1] if want to control hoyer sparsity for both layers.\n",
    "wsc_flag = [1,1]\n",
    "\n",
    "\n",
    "### Set pretrained model\n",
    "### 'RandomInit', 'FinetuneAE', 'FixAE', 'FinetuneRBM', 'FixRBM'\n",
    "### [Model, pretrained 1st layer sparsity, denoising level]\n",
    "\n",
    "pretrain_type_list = [\n",
    "\n",
    "    \n",
    "#     ['RandomInit', None, 0], \n",
    "    \n",
    "#     ['FixRBM', 0.5, 0], \n",
    "#     ['FixAE', 0.5, 0], \n",
    "#     ['FixRBM', 0.8, 0], \n",
    "#     ['FixAE', 0.8, 0], \n",
    "#     ['FixRBM', 0.9, 0], \n",
    "#     ['FixAE', 0.9, 0], \n",
    "\n",
    "    \n",
    "#     ['FinetuneAE', 0.5, 0], \n",
    "#     ['FinetuneRBM', 0.5, 0],\n",
    "    ['FinetuneAE', 0.8, 0], \n",
    "#     ['FinetuneRBM', 0.8, 0],\n",
    "#     ['FinetuneAE', 0.9, 0], \n",
    "#     ['FinetuneRBM', 0.9, 0],\n",
    "]\n",
    "\n",
    "#Randomseed for all\n",
    "random_seed = config['random_state']\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cf2a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: 1\n",
      "Current cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"gpu_num\"])\n",
    "\n",
    "print('Available devices:', torch.cuda.device_count())\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f9d46",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bfbc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TL4fMRI_load_sbj import load_sbj\n",
    "\n",
    "load = load_sbj(pretrain_type_list)\n",
    "is_subject_valid = load.is_subject_valid\n",
    "get_sbj_task_data = load.get_sbj_task_data\n",
    "load_data = load.load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "778fce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 75.88it/s]\n",
      " 56%|█████▌    | 5/9 [00:00<00:00, 44.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 training subjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 43.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 test subjects\n",
      "Bring SOCIAL task data\n",
      "Training data shape : input-(5360, 5000), label-(5360, 2)\n",
      "Training data shape : input-(4824, 5000), label-(4824, 2)\n"
     ]
    }
   ],
   "source": [
    "### Bring data according to training settings\n",
    "\n",
    "# Load n data\n",
    "train_samples, train_labels, train_subjects, test_samples, test_labels, test_subjects, task_str = load_data(config=config)\n",
    "\n",
    "\n",
    "train_x = train_samples\n",
    "train_y = train_labels if config['output_size'] ==2 else train_labels.reshape(-1,1)\n",
    "\n",
    "test_x = test_samples\n",
    "test_y = test_labels if config['output_size'] ==2 else test_labels.reshape(-1,1)\n",
    "\n",
    "del(train_samples)\n",
    "del(train_labels)\n",
    "del(test_samples)\n",
    "del(test_labels)\n",
    "\n",
    "gc.collect()\n",
    "        \n",
    "print(f\"Bring {task_str} task data\")\n",
    "print(f\"Training data shape : input-{train_x.shape}, label-{train_y.shape}\")\n",
    "print(f\"Training data shape : input-{test_x.shape}, label-{test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb3da990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "class train_dataset(Dataset): \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_train = torch.from_numpy(self.X_train[idx]).type(torch.FloatTensor)\n",
    "        y_train = torch.from_numpy(self.y_train[idx]).type(torch.LongTensor) if config['output_size'] >2 else torch.from_numpy(self.y_train[idx]).type(torch.FloatTensor)\n",
    "\n",
    "        return X_train, y_train\n",
    "    \n",
    "# Test dataset\n",
    "class test_dataset(Dataset): \n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_test)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_test = torch.from_numpy(self.X_test[idx]).type(torch.FloatTensor)\n",
    "        y_test = torch.from_numpy(self.y_test[idx]).type(torch.LongTensor) if config['output_size'] >2 else torch.from_numpy(self.y_test[idx]).type(torch.FloatTensor)\n",
    "        \n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2c4c0",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbf3aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping_acc:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_acc_max = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_acc, model):\n",
    "\n",
    "        score = val_acc\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation Acc increased ({self.val_acc_max:.6f} --> {val_acc:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_acc_max = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32158485",
   "metadata": {},
   "source": [
    "## Repeated group kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeb18528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGroupKFold(_BaseKFold):\n",
    "    def __init__(self, n_splits=5, *, shuffle=False, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle,\n",
    "                         random_state=random_state)\n",
    "\n",
    "    def _iter_test_indices(self, X, y, groups):\n",
    "        if groups is None:\n",
    "            raise ValueError(\"The 'groups' parameter should not be None.\")\n",
    "        groups = check_array(groups, ensure_2d=False, dtype=None)\n",
    "        \n",
    "        unique_groups, groups = np.unique(groups, return_inverse=True)\n",
    "        n_groups = len(unique_groups)\n",
    "\n",
    "        if self.n_splits > n_groups:\n",
    "            raise ValueError(\"Cannot have number of splits n_splits=%d greater\"\n",
    "                             \" than the number of groups: %d.\"\n",
    "                             % (self.n_splits, n_groups))\n",
    "\n",
    "        # Weight groups by their number of occurrences\n",
    "        n_samples_per_group = np.bincount(groups)\n",
    "\n",
    "        # Distribute the most frequent groups first\n",
    "        indices = np.argsort(n_samples_per_group)[::-1]\n",
    "        \n",
    "        if self.shuffle:\n",
    "            rng = check_random_state(self.random_state)\n",
    "            for n_sample in np.unique(n_samples_per_group):\n",
    "                same_n_indices_index = np.where(n_samples_per_group == n_sample)[0]\n",
    "                target_chunk = indices[same_n_indices_index]\n",
    "                rng.shuffle(target_chunk)\n",
    "                indices[same_n_indices_index] = target_chunk\n",
    "\n",
    "        n_samples_per_group = n_samples_per_group[indices]\n",
    "\n",
    "        # Total weight of each fold\n",
    "        n_samples_per_fold = np.zeros(self.n_splits)\n",
    "\n",
    "        # Mapping from group index to fold index\n",
    "        group_to_fold = np.zeros(len(unique_groups))\n",
    "\n",
    "        # Distribute samples by adding the largest weight to the lightest fold\n",
    "        for group_index, weight in enumerate(n_samples_per_group):\n",
    "            lightest_fold = np.argmin(n_samples_per_fold)\n",
    "            n_samples_per_fold[lightest_fold] += weight\n",
    "            group_to_fold[indices[group_index]] = lightest_fold\n",
    "\n",
    "        indices = group_to_fold[groups]\n",
    "\n",
    "        for f in range(self.n_splits):\n",
    "            yield np.where(indices == f)[0]\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        return super().split(X, y, groups)\n",
    "    \n",
    "\n",
    "class RepeatedGroupKFold(_RepeatedSplits):\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self, *, n_splits=5, n_repeats=10, random_state=None):\n",
    "        super().__init__(\n",
    "            CustomGroupKFold, n_repeats=n_repeats,\n",
    "            random_state=random_state, n_splits=n_splits)\n",
    "        \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_repeats = self.n_repeats\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        for idx in range(n_repeats):\n",
    "            cv = self.cv(random_state=rng, shuffle=True,\n",
    "                         **self.cvargs)\n",
    "            for train_index, test_index in cv.split(X, y, groups):\n",
    "                yield train_index, test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13cd798",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d991a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_activation_func():\n",
    "    if config['activation'] == None:\n",
    "        activ= False\n",
    "    if config['activation'] == 'tanh':\n",
    "        activ= nn.Tanh()\n",
    "    if config['activation'] == 'relu':\n",
    "        activ= nn.ReLU()\n",
    "    if config['activation'] == 'lkrelu':\n",
    "        activ= nn.LeakyReLU()\n",
    "    if config['activation'] == 'elu':\n",
    "        activ= nn.ELU()\n",
    "    if config['activation'] == 'prelu':\n",
    "        activ= nn.PReLU()\n",
    "        \n",
    "    if config['activation2'] == None:\n",
    "        activ2= False\n",
    "    if config['activation2'] == 'tanh':\n",
    "        activ2= nn.Tanh()\n",
    "    if config['activation2'] == 'relu':\n",
    "        activ2= nn.ReLU()\n",
    "    if config['activation2'] == 'lkrelu':\n",
    "        activ2= nn.LeakyReLU()\n",
    "    if config['activation2'] == 'elu':\n",
    "        activ2= nn.ELU()\n",
    "    if config['activation2'] == 'prelu':\n",
    "        activ2= nn.PReLU()\n",
    "        \n",
    "    return activ, activ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c61eb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune / RandomInit model\n",
    "class HSCmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HSCmodel, self).__init__()\n",
    "        self.fc1 = nn.Linear(52470, config['comb']['comb_num'][0])\n",
    "        self.fc2 = nn.Linear(config['comb']['comb_num'][0], config['comb']['comb_num'][1])\n",
    "        self.fc3 = nn.Linear(config['comb']['comb_num'][1], config['output_size'])\n",
    "        self.activ,self.activ2 = set_activation_func()\n",
    "        if config['norm']['type'] == 'batch' and config['norm']['first']==True:\n",
    "            self.bn1 = nn.BatchNorm1d(config['comb']['comb_num'][0])\n",
    "        if config['norm']['type'] == 'layer' and config['norm']['first']==True:\n",
    "            self.ln1 = nn.LayerNorm(config['comb']['comb_num'][0])\n",
    "        if config['norm']['type'] == 'batch' and config['norm']['second']==True:\n",
    "            self.bn2 = nn.BatchNorm1d(config['comb']['comb_num'][1])\n",
    "        if config['norm']['type'] == 'layer' and config['norm']['second']==True:\n",
    "            self.ln2 = nn.LayerNorm(config['comb']['comb_num'][1])\n",
    "        self.dropout = nn.Dropout(config['dropout_rate'])\n",
    "        self.weights_init()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if config['norm']['type'] == 'batch' and config['norm']['first']==True:\n",
    "            x = self.bn1(x)\n",
    "        if config['norm']['type'] == 'layer' and config['norm']['first']==True:\n",
    "            x = self.ln1(x)\n",
    "        if config['activation'] != None:\n",
    "            x = self.activ(x)\n",
    "        x = self.fc2(x)\n",
    "        if config['norm']['type'] == 'batch' and config['norm']['second']==True:\n",
    "            x = self.bn2(x)\n",
    "        if config['norm']['type'] == 'layer' and config['norm']['second']==True:\n",
    "            x = self.ln2(x)\n",
    "        if config['activation2'] != None:\n",
    "            x = self.activ2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # He initialization for ReLU\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                # Bias initialization\n",
    "                nn.init.normal_(m.bias, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af681bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_per_pretrained(p_model):\n",
    "    \n",
    "    #Init DNN model\n",
    "    print('======================')\n",
    "    print('Model building...')\n",
    "    \n",
    "    model = HSCmodel()\n",
    "    \n",
    "    if 'AE' in p_model[0]:\n",
    "        hsp_1st_layer = str(p_model[1]).replace('.','') #1st layer sparsity\n",
    "        pretrained_model = sio.loadmat(os.getcwd()+\"/AE/AE_model/AE_hsp{}_dns0.mat\".format(hsp_1st_layer))\n",
    "        pretrained_w = pretrained_model['encoder.weight']\n",
    "        pretrained_b = pretrained_model['encoder.bias']\n",
    "        del pretrained_model\n",
    "        model.fc1.state_dict()['weight'].data.copy_(torch.from_numpy(pretrained_w.T))\n",
    "        model.fc1.state_dict()['bias'].data.copy_(torch.from_numpy(pretrained_b.reshape(config['comb']['comb_num'][0],)))\n",
    "\n",
    "    if 'RBM' in p_model[0]:\n",
    "        if p_model[1] == 0.5:\n",
    "            pretrained_model = sio.loadmat(os.getcwd()+'/RBM/RBM_model/RBM_hsp05_dns0.mat')\n",
    "            pretrained_w = pretrained_model['rbm_w']\n",
    "            pretrained_b = pretrained_model['rbm_c']\n",
    "\n",
    "        if p_model[1] == 0.8:\n",
    "            pretrained_model = sio.loadmat(os.getcwd()+'/RBM/RBM_model/RBM_hsp08_dns0.mat')\n",
    "            pretrained_w = pretrained_model['tmp_w']\n",
    "            pretrained_b = pretrained_model['tmp_c']\n",
    "\n",
    "        if p_model[1] == 0.9:\n",
    "            pretrained_model = sio.loadmat(os.getcwd()+'/RBM/RBM_model/RBM_hsp09_dns0_w.mat')\n",
    "            pretrained_w = pretrained_model['tmp_w']\n",
    "            pretrained_b = sio.loadmat('/RBM/RBM_model/RBM_hsp09_dns0_bias')['c_gtd']        \n",
    "        del pretrained_model\n",
    "\n",
    "        model.fc1.state_dict()['weight'].data.copy_(torch.from_numpy(pretrained_w))\n",
    "        model.fc1.state_dict()['bias'].data.copy_(torch.from_numpy(pretrained_b.reshape(5000,)))\n",
    "        \n",
    "    # pretrained weight,bias fixed / finetune\n",
    "    model.fc1.weight.requires_grad = False if 'Fix' in p_model[0] else True\n",
    "    model.fc1.bias.requires_grad = False if 'Fix' in p_model[0] else True\n",
    "    \n",
    "    print('Gradient update for 1st layer')\n",
    "    print('Weight : ',model.fc1.weight.requires_grad)\n",
    "    print('Bias : ',model.fc1.bias.requires_grad)\n",
    "    print('======================')\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a2c40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save fold learning data & parameters\n",
    "def save_fold(tgs_folder):\n",
    "\n",
    "    np.savez_compressed(\n",
    "        pth.join(tgs_folder, '{}fold_learning_curve_{}'.format(config['num_fold'],\n",
    "                                                              fold+1)), \n",
    "        loss=outer_train_loss, \n",
    "        val_loss=outer_valid_loss,\n",
    "        test_loss=outer_test_loss,\n",
    "        acc=outer_train_acc, \n",
    "        val_acc=outer_valid_acc,  \n",
    "        test_acc=outer_test_acc,\n",
    "        lr = outer_lr_list,\n",
    "        HSC_hsp1=layer_1_sp,\n",
    "        HSC_beta1=layer_1_b,\n",
    "        HSC_hsp2=layer_2_sp,\n",
    "        HSC_beta2=layer_2_b,\n",
    "        train_sbj = fold_train_sbj,\n",
    "        val_sbj = fold_valid_sbj,\n",
    "        model_epoch_idx = model_epoch,\n",
    "        elapsed_time = elapsed_time\n",
    "    )\n",
    "    print('Learning curve saved at {}'.format(tgs_folder))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a608778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data save module\n",
    "from TL4fMRI_save_data import save_data\n",
    "\n",
    "sv = save_data(config)\n",
    "\n",
    "save_file = sv.save_file\n",
    "make_sp_folder = sv.make_sp_folder\n",
    "save_parameters = sv.save_parameters\n",
    "save_plot_during_training = sv.save_plot_during_training\n",
    "\n",
    "# Import hoyer sparsity control module\n",
    "from TL4fMRI_sparsity_control import HSP\n",
    "\n",
    "hsp = HSP(config,wsc_flag,pretrain_type_list)\n",
    "\n",
    "sparsity_control = hsp.sparsity_control\n",
    "Hoyers_sparsity_control = hsp.Hoyers_sparsity_control\n",
    "init_hsp = hsp.init_hsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45841532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current pretrained model is [FixRBM-0.8]\n",
      "Target sparsity is [0.8, 0.5]\n",
      "HSC type is layer\n",
      "Save file path : /users/hjd/hsp_results/DNN_TL4fMRI/Transfer_learning/SOCIAL/FixRBM_0.8-0.5\n",
      "Save file path : /users/hjd/hsp_results/DNN_TL4fMRI/Transfer_learning/SOCIAL/FixRBM_0.8-0.5\n",
      "-------------------Fold 1-------------------\n",
      "======================\n",
      "Model building...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5000) must match the size of tensor b (52470) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-791537bf9b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m#Build DNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_per_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d77df33d9834>\u001b[0m in \u001b[0;36mbuild_model_per_pretrained\u001b[0;34m(p_model)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# pretrained 모델 weight, bias 카피\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5000) must match the size of tensor b (52470) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "lr_curve_list = []\n",
    "config['output_size'] = config['classes_for_task'][config['target_task']]  \n",
    "\n",
    "\n",
    "#Target sparsity별\n",
    "for tgs in target_h_list: #tgs[0]:1st layer, tgs[1]:2nd layer\n",
    "    #pretrained 모델 별\n",
    "    for p_model in pretrain_type_list: #p_model[0]:model name, p_model[1]:first layer hsp\n",
    "        if tgs[0] != p_model[1]:\n",
    "            if 'AE' in p_model[0] or 'RBM' in p_model[0]:\n",
    "                continue\n",
    "            elif 'Random' in p_model[0]:\n",
    "                pass\n",
    "            else:\n",
    "                print('Invalid pretrain model')\n",
    "                break\n",
    "\n",
    "        print('Current pretrained model is [{}-{}]'.format(p_model[0],p_model[1]))\n",
    "        print('Target sparsity is {}'.format(tgs))\n",
    "        print('HSC type is {}'.format(config['hsc']['type']))\n",
    "        \n",
    "        #Make result folder\n",
    "        output_folder = save_file(p_model,tgs)\n",
    "        save_lr_path = make_sp_folder(p_model,tgs)\n",
    "        save_parameters(p_model,save_lr_path)\n",
    "\n",
    "        #Split in group K fold\n",
    "        n_splits = config['num_fold']\n",
    "        gkf = RepeatedGroupKFold(n_splits=n_splits,\n",
    "                                 n_repeats=config['num_repeat_of_fold'],\n",
    "                                 random_state=config['random_state'])\n",
    "\n",
    "\n",
    "        for fold, (train_idx, valid_idx) in enumerate(gkf.split(train_x, train_y, groups=train_subjects)):\n",
    "            print(f'-------------------Fold {fold+1}-------------------')\n",
    "            fold_train_sbj = np.unique(train_subjects[train_idx])\n",
    "            fold_valid_sbj = np.unique(train_subjects[valid_idx])\n",
    "            fold_train_x = train_x[train_idx]\n",
    "            fold_train_y = train_y[train_idx]\n",
    "            fold_valid_x = train_x[valid_idx]\n",
    "            fold_valid_y = train_y[valid_idx]\n",
    "\n",
    "            #early stopping, learning rate custom annealing\n",
    "            ES_switch = [False, 0]\n",
    "            LR_switch = [False, False, 0]\n",
    "\n",
    "            model_epoch = 0\n",
    "\n",
    "            # Learning curve for 1 epochs\n",
    "            outer_train_loss = []\n",
    "            outer_valid_loss = []\n",
    "            outer_test_loss = []\n",
    "            outer_train_acc = []\n",
    "            outer_valid_acc = []\n",
    "            outer_test_acc = []\n",
    "            outer_lr_list = []\n",
    "            layer_1_sp = []\n",
    "            layer_2_sp = []\n",
    "            layer_1_b = []\n",
    "            layer_2_b = []\n",
    "\n",
    "\n",
    "            #Make train/valid/test dataset & dataloader accordingly\n",
    "            fold_train_dataset = train_dataset(fold_train_x, fold_train_y)\n",
    "            fold_valid_dataset = train_dataset(fold_valid_x, fold_valid_y)\n",
    "            fold_test_dataset = test_dataset(test_x, test_y)\n",
    "\n",
    "            train_loader = DataLoader(fold_train_dataset, batch_size=config['batch_size'], \n",
    "                                            shuffle=True, drop_last=True,num_workers=4)\n",
    "            valid_loader = DataLoader(fold_valid_dataset, batch_size=config['batch_size'], \n",
    "                                            shuffle=True, drop_last=True,num_workers=4)\n",
    "            test_loader = DataLoader(fold_test_dataset, batch_size=config['batch_size'], \n",
    "                                            shuffle=True, drop_last=True,num_workers=4)\n",
    "\n",
    "            del fold_train_x\n",
    "            del fold_train_y\n",
    "            del fold_valid_x\n",
    "            del fold_valid_y\n",
    "\n",
    "\n",
    "    \n",
    "            #Build DNN model\n",
    "            model = build_model_per_pretrained(p_model)\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "            #Init optimizer & scheduler\n",
    "            optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'], \n",
    "                              weight_decay=config['hsc']['l2'], momentum=config['momentum'], nesterov=True)\n",
    "            criterion =nn.BCEWithLogitsLoss() if config['output_size'] ==2 else nn.CrossEntropyLoss() #nn.BCEWithLogitsLoss() #nn.CrossEntropyLoss()\n",
    "            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor = config['anneal_factor'], patience = config['lr_patience'])\n",
    "\n",
    "\n",
    "            #Init HSP\n",
    "            if 'Fix' in p_model[0]:\n",
    "                hsp_val, beta_val, hsp_list, beta_list = 0,0,[],[]\n",
    "            else:\n",
    "                hsp_val, beta_val, hsp_list, beta_list = init_hsp()\n",
    "\n",
    "            #EarlyStopping\n",
    "            early_stopping = EarlyStopping_acc(patience = config['es_patience']\n",
    "                                           , verbose = True\n",
    "                                           , path = save_lr_path + '/{}fold.pt'.format(fold+1))\n",
    "\n",
    "            fold_time = dt.now(tz)\n",
    "            \n",
    "            #Train begins here\n",
    "            for epoch in range(0,config['num_epoch']):\n",
    "                print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "                #Time calculate\n",
    "                cur_time = dt.now(tz)\n",
    "\n",
    "                #Train\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                total = 0\n",
    "                train_acc = 0\n",
    "                train_correct = 0\n",
    "                count = 1\n",
    "\n",
    "\n",
    "                if 'Finetune' in p_model[0]:\n",
    "                    model.fc1.weight.requires_grad = True\n",
    "                    model.fc1.bias.requires_grad = True\n",
    "\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                outer_lr_list.append(current_lr)\n",
    "                print('Learning rate : ', current_lr)\n",
    "\n",
    "                for input_, target in train_loader:\n",
    "                    optimizer.zero_grad() #gradient reset\n",
    "                    if config['output_size'] >2:\n",
    "                        target = target.reshape(config['batch_size'],)\n",
    "                    input_, target = input_.to(device), target.to(device)\n",
    "                    output = model(input_) \n",
    "                    loss = criterion(output, target)\n",
    "\n",
    "                    #Calculate L1 with hoyer sparsity control applied\n",
    "                    l1_term,hsp_val,beta_val= sparsity_control(model, hsp_val, \n",
    "                                                 beta_val, hsp_list, beta_list, tgs)\n",
    "                    cost = loss + l1_term #Calculate totla cost\n",
    "                    train_loss += loss.item()\n",
    "                    cost.backward() #cost backpropagate\n",
    "                    optimizer.step() #update optimizer\n",
    "                    total += output.size(0)\n",
    "                    _, pred = torch.max(output.data, 1)\n",
    "                    if config['output_size']==2:\n",
    "                        _, target = torch.max(target,1)\n",
    "                    train_correct += (pred == target).sum().item()\n",
    "                    count+=1\n",
    "                print(\"L1 regularization gradient updating status :\", l1_term.requires_grad)\n",
    "                if 'Fix' in p_model[0]:\n",
    "                    if type(hsp_val) == float:\n",
    "                        hsp_list.append(hsp_val)\n",
    "                    if type(hsp_val) == torch.Tensor:\n",
    "                        hsp_list.append(hsp_val.cpu().detach().numpy())\n",
    "                    if type(beta_val) == float:\n",
    "                        beta_list.append(beta_val)\n",
    "                    if type(beta_val) == torch.Tensor:\n",
    "                        beta_list.append(beta_val.cpu().detach().numpy())\n",
    "\n",
    "                    print(f'Current layer2 sparsity is... {np.round(hsp_list[epoch],3)}')\n",
    "                    print(f'Current layer2 beta is... {np.round(beta_list[epoch],6)}')\n",
    "                else:\n",
    "                    print(type(hsp_val[0]),type(hsp_val[1]),type(beta_val[0]),type(hsp_val[1]))\n",
    "                    hsp_list[epoch][0] = hsp_val[0].cpu().detach().numpy()\n",
    "                    beta_list[epoch][0] = beta_val[0]\n",
    "                    hsp_list[epoch][1] = hsp_val[1].cpu().detach().numpy()\n",
    "\n",
    "                    if type(beta_val[0]) == float:\n",
    "                        beta_list[epoch][0] = beta_val[0] #.cpu().detach().numpy()\n",
    "                    if type(beta_val[0]) == torch.Tensor:\n",
    "                        beta_list[epoch][0] = beta_val[0].cpu().detach().numpy()\n",
    "                    if type(beta_val[1]) == float:\n",
    "                        beta_list[epoch][1] = beta_val[1] #.cpu().detach().numpy()\n",
    "                    if type(beta_val[1]) == torch.Tensor:\n",
    "                        beta_list[epoch][1] = beta_val[1].cpu().detach().numpy()\n",
    "                \n",
    "                #Print HSP,beta status\n",
    "                print(f'Current layer1 sparsity is... {np.round(hsp_list[epoch][0],3)}')\n",
    "                print(f'Current layer1 beta is... {np.round(beta_list[epoch][0],6)}')\n",
    "                print(f'Current layer2 sparsity is... {np.round(hsp_list[epoch][1],3)}')\n",
    "                print(f'Current layer2 beta is... {np.round(beta_list[epoch][1],6)}')\n",
    "                train_loss /= count\n",
    "                train_acc = 100 * train_correct / total\n",
    "\n",
    "                #Save loss & acc for each epoch\n",
    "                outer_train_acc.append(train_acc)\n",
    "                outer_train_loss.append(train_loss)\n",
    "                print(f'Train_acc : {round(train_acc,3)}')\n",
    "                print(f'Train_loss : {round(train_loss,3)}')\n",
    "\n",
    "                #Validation\n",
    "                model.eval()\n",
    "                valid_loss = 0\n",
    "                valid_acc = 0\n",
    "                valid_correct = 0\n",
    "                total = 0\n",
    "                count = 1\n",
    "                with torch.no_grad():\n",
    "                    for input_, target in valid_loader:\n",
    "                        if config['output_size'] >2: \n",
    "                            target = target.reshape(config['batch_size'],)\n",
    "                        input_, target = input_.to(device), target.to(device)\n",
    "                        output = model(input_)\n",
    "                        loss = criterion(output, target)\n",
    "                        valid_loss += loss.item()\n",
    "                        total += output.size(0)\n",
    "                        _, pred = torch.max(output.data, 1)\n",
    "                        if config['output_size']==2:\n",
    "                            _, target = torch.max(target,1)\n",
    "                        valid_correct += (pred == target).sum().item()\n",
    "                        count +=1\n",
    "                    valid_loss /= count\n",
    "                    valid_acc = 100 * valid_correct / total\n",
    "                    \n",
    "                    #Save loss & acc for each epoch\n",
    "                    outer_valid_loss.append(valid_loss)\n",
    "                    outer_valid_acc.append(valid_acc)\n",
    "                    print(f'Validation_acc : {round(valid_acc,3)}')\n",
    "                    print(f'Validation_loss : {round(valid_loss,3)}')\n",
    "\n",
    "\n",
    "                #Test\n",
    "                model.eval()\n",
    "                test_loss = 0\n",
    "                test_acc = 0\n",
    "                test_correct = 0\n",
    "                total = 0\n",
    "                count = 1\n",
    "                with torch.no_grad():\n",
    "                    for input_, target in test_loader:\n",
    "                        if config['output_size'] >2:\n",
    "                            target = target.reshape(config['batch_size'],)\n",
    "                        input_, target = input_.to(device), target.to(device)\n",
    "                        output = model(input_)\n",
    "                        loss = criterion(output, target)\n",
    "                        test_loss += loss.item()\n",
    "                        total += output.size(0)\n",
    "                        _, pred = torch.max(output.data, 1)\n",
    "                        if config['output_size']==2:\n",
    "                            _, target = torch.max(target,1)\n",
    "                        test_correct += (pred == target).sum().item()\n",
    "                        count +=1\n",
    "                    test_loss /= count\n",
    "                    test_acc = 100 * test_correct / total\n",
    "                    \n",
    "                    #Save loss & acc for each epoch\n",
    "                    outer_test_loss.append(test_loss)\n",
    "                    outer_test_acc.append(test_acc)\n",
    "                    print(f'Test_acc : {round(test_acc,3)}')\n",
    "                    print(f'Test_loss : {round(test_loss,3)}')          \n",
    "                    \n",
    "                    print(\"Time spent for this epoch : {}\".format(dt.now(tz)-cur_time))\n",
    "\n",
    "                #Start earlystopping after reaching target sparsity\n",
    "                if np.mean(hsp_list[epoch][0]) >= tgs[0]-0.001 and np.mean(hsp_list[epoch][1])>=tgs[1]-0.001:\n",
    "                    ES_switch[0] = True\n",
    "                    ES_switch[1]+=1\n",
    "                    if ES_switch[1]==1:\n",
    "                        print('EarlyStopping count Start!')\n",
    "                if ES_switch[0] == True and ES_switch[1] < config['es_patience']/10:\n",
    "                    print(\"Wait for stabilize\")\n",
    "                if ES_switch[0] == True and ES_switch[1] >= config['es_patience']/10:\n",
    "                    early_stopping(valid_acc, model)\n",
    "                    scheduler.step(valid_loss)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(\"Early stopping!!!\")\n",
    "                    model_epoch = epoch-config['es_patience']\n",
    "                    break\n",
    "                \n",
    "                #Save figure for current learning status\n",
    "                save_plot_during_training(p_model,tgs,save_lr_path,hsp_list,fold,epoch,\n",
    "                                         outer_train_acc,outer_valid_acc,outer_test_acc,outer_lr_list,model_epoch)\n",
    "                print('=======================')\n",
    "\n",
    "            elapsed_time = dt.now(tz)-fold_time\n",
    "\n",
    "            for i in range(0,epoch+1):\n",
    "                layer_1_sp.append(hsp_list[i][0])\n",
    "                layer_2_sp.append(hsp_list[i][1])\n",
    "                layer_1_b.append(beta_list[i][0])\n",
    "                layer_2_b.append(beta_list[i][1])\n",
    "\n",
    "            clear_output()\n",
    "            save_fold(save_lr_path)\n",
    "        lr_curve_list.append(save_lr_path)\n",
    "\n",
    "        #Clear memory after training\n",
    "        del outer_train_loss\n",
    "        del outer_valid_loss\n",
    "        del outer_test_loss\n",
    "        del outer_train_acc\n",
    "        del outer_valid_acc\n",
    "        del outer_test_acc\n",
    "        del outer_lr_list\n",
    "        del layer_1_sp\n",
    "        del layer_2_sp\n",
    "        del layer_1_b\n",
    "        del layer_2_b\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(save_lr_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
